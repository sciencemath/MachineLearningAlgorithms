{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d390b6a8-9be9-485b-bdaa-047b3c5c4596",
   "metadata": {},
   "source": [
    "### Exponential Linear Unit as an activation function outperformed ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23232c67-e8b9-4722-b125-b1f6421aa098",
   "metadata": {},
   "source": [
    "it takes on negative numbers allowing the training to not have vanishing gradients\n",
    "\n",
    "nonzero gradient for z < 0 so no dead neurons\n",
    "\n",
    "Helps gradient descent beacause the function is smooth everywhere (doesn't bounch as much)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b7042-55b9-4544-9c82-b66ec2b21595",
   "metadata": {},
   "source": [
    "### SELU is a scaled ELU activation function\n",
    "\n",
    "Cannot use regularization techiques like l1 or l2, max-norm, batch-norm, regular dropout\n",
    "\n",
    "self-normalizing is only guarenteed with plain MLPs\n",
    "\n",
    "input features must be standardized: mean 0 and SD of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4936916-4b8b-4d96-a9b8-0bfa1535e7f9",
   "metadata": {},
   "source": [
    "### GELU Gaussian Error Linear Units looks like ReLU but is smooth all over makes gradient descent easier to fit complex problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35eb22-112e-42d1-ac91-bc4875e780d5",
   "metadata": {},
   "source": [
    "SiLU activation outperformed GELU (Swish, β to scale sigmoid function's input)\n",
    "\n",
    "Mish is smooth, nonconvex, and nonmonotonix variant of ReLU and outperformed Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fcd6b1-4025-4880-864b-7197f7f3cd3b",
   "metadata": {},
   "source": [
    "### ReLU is a good default (hardware accelerators provide ReLU-specific optimizations)\n",
    "\n",
    "Switch is better default for more complex tasks, Mish may give slightly better results\n",
    "\n",
    "for runtime latency LeakyReLU or Parameterized Leaky ReLU for complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e4649-8b3c-43bc-86f1-b2912b79ea1a",
   "metadata": {},
   "source": [
    "### Batch Normalization (BN) reduce the danger of vanishing/exploding gradients\n",
    "\n",
    "adding an operation in model before or after activation function of each hidden layer. Zero-centers and normalizess each input, the nscales and shifts (using two new parameter vectors per layer scale and shifting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b801a1-15cc-48df-8782-77d8ad579b42",
   "metadata": {},
   "source": [
    "No need for `StandardScaler` or `Normalization` if BN is first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06964f-cfe7-47f1-bf1d-75fe46721af4",
   "metadata": {},
   "source": [
    "#### **1. Compute the Mini-Batch Mean**\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "$$\n",
    "\n",
    "#### **2. Compute the Mini-Batch Variance**\n",
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "$$\n",
    "\n",
    "#### **3. Normalize the Inputs**\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "where $ \\epsilon $ is a small constant to prevent division by zero. (smoothing term)\n",
    "\n",
    "#### **4. Scale and Shift**\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "where:\n",
    "- $ \\gamma $ (scale) and $ \\beta $ (shift) are **learnable parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674967a-65e8-4a89-9295-fc9a425fe93b",
   "metadata": {},
   "source": [
    "if we want to test predicitions for individual instances rather than batches we only have the batch mean/SD so:\n",
    "\n",
    "most implementations of batch normalization estimate final statistics during training by using a moving\n",
    "average of the layer's input means and standard deviations, Keras does this automatically.\n",
    "\n",
    "it's possible to fuse the BN layer with the previous layer after training, avoiding the runtime penalty.\n",
    "This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. \n",
    "\n",
    "previous layer computes XW + b, then the BN layer will compute γ⊗(XW + b – μ) / σ + β (ignoring the smoothing term ε). W' = γ⊗W /\n",
    "σ and b′ = γ⊗(b – μ) / σ + β, the equation simplifies to XW' + b'. replace the previous layer's weights and biases (W and b) with the updated weightsand biases (W' and b'), we can get rid of the BN layer (⊗ element-wise multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15322935-32e2-49fa-a804-9968d3f70bf2",
   "metadata": {},
   "source": [
    "For small networks might not have much impact but you can see for deeper networks this can make a huge difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a172f1a-133d-4b5f-8354-a920fb65cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631576a1-1f5c-4733-8f15-02f199dc02c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0589668-0ab0-441a-92e9-98684071e942",
   "metadata": {},
   "source": [
    "first BN layer: 3,136 parameters = 4 × 784 (γ, β, μ, and σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d02ee-349f-44b3-9771-d7c2dda591e1",
   "metadata": {},
   "source": [
    "μ and σ, are the moving averages not trainable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9861457f-8b51-4c2c-a81c-44fc7f7767ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gamma', True),\n",
       " ('beta', True),\n",
       " ('moving_mean', False),\n",
       " ('moving_variance', False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ea9449-5be1-429f-8fd7-bca0aa1d4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is some debate to put the BN before/after activation function\n",
    "# BN layer includes one offset parameter per input, you can remove the bias term from the previous layer \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbaa4dc-2e46-435c-8c06-6f4150f9b9e1",
   "metadata": {},
   "source": [
    "Hyperparams for `BatchNormalization` momentum:\n",
    "\n",
    "$\\hat{v}$: running average\n",
    "\n",
    "$\\hat{v}$ <- $\\hat{v}$ × momentum + v × (1 − momentum)\n",
    "\n",
    "axis:\n",
    "\n",
    "defaults to -1, last axis (using means and SD computed across other axes)\n",
    "if you you want to treat each pixel independently axis=[1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5850ed5-bba3-4da6-90d0-3c8ae1d7bcd3",
   "metadata": {},
   "source": [
    "### Gradient Clipping mitigates the exploding gradients (setting a threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ddfca3-1364-463f-a5cf-5037c4d7f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizer.SGD(clipvalue=1.0)\n",
    "# model.compile([...], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932ce1e-828a-4b5f-9d62-64bc5d4a2316",
   "metadata": {},
   "source": [
    "clipping should be done by setting the threshold of the norm\n",
    "usually we want to clip between -1.0 and 1.0\n",
    "\n",
    "we don't want to change the orientation of gradient example: [0.9, 100.0] to [0.9, 1.0]\n",
    "clipnorm=1.0 will do this instead [0.9, 100.0] -> [0.00899, 0.9999] keeping orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483939a1-ecf9-46b6-8790-a2b704a68c1a",
   "metadata": {},
   "source": [
    "### Resusing Layers\n",
    "\n",
    "You can re-use lower layers of a Deep NN by freezing them so backpropigation only performs on top layers instead of dropping them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753cb3d-ae4c-43c4-8c34-bc167ca6461b",
   "metadata": {},
   "source": [
    "### transfer learning\n",
    "\n",
    "say you have images of T-shirts and sandels and want to use the training from the fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c29231e-5a7f-437e-a8b4-de9e4314c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model_A was trained on fashion dataset\n",
    "# model_A = tf.keras.models.load_model(\"my_model_A\")\n",
    "# model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
    "# model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950b2611-96e1-458b-8bdf-1029c43d83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "# model_A_clone.set_weights(model_A.get_weights()) # otherwise weights are initialized randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ea9f5c-0803-4d59-80f9-53ddb42b1686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    for layer in model_B_on_A.layers[:-1]:\\n        layer.trainable = False\\n\\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\\n    model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to avoid large error gradients that may wreck the reused wweights\n",
    "# freeze the reused layers during first few epochs\n",
    "\"\"\"\n",
    "    for layer in model_B_on_A.layers[:-1]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "    model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144622d-f395-4965-b1d3-783666a222f3",
   "metadata": {},
   "source": [
    "now unfreeze reused layers and continue training, good idea to reduce learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b53e457-a25c-458e-8c4f-b832fdb5500c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhistory = model_B_on_A.fit(X_trian_B, y_train_B, epochs=4,\\n                           validation_data=(X_valid_B, y_valid_B))\\n\\nfor layer in model_B_on_A.layers[:-1]:\\n    layer.trainable = True\\n\\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\\nmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\\n                           validation_data=(X_valid_B, y_valid_B))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "history = model_B_on_A.fit(X_trian_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "676f5e9f-7d0a-4514-9228-bdba03641f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf0075-7ce9-452a-9227-8180135d7e26",
   "metadata": {},
   "source": [
    "Transfer learning only works well with Deep convolutional neural networks, not small dense networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b40f8b-2c42-41be-bb5a-c5931246372b",
   "metadata": {},
   "source": [
    "### Unsupervised pretraining\n",
    "\n",
    "you can use this if you don't have much labeled training data, and you cannot find a model trained on a similar task, could use autoencoders or GANs, and the finial task is just on the labeled data using supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b4a98-33bc-43ba-ae28-b3cdf6a76d26",
   "metadata": {},
   "source": [
    "### optimizers\n",
    "\n",
    "(Regular gradient descent will take small steps when slope is gentle and big steps when slope is steep but will never pick up speed)\n",
    "\n",
    "- momentum\n",
    "- Nesterov accelerated gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abbd34-b8a4-45e3-bdc2-6a0d8d831fa1",
   "metadata": {},
   "source": [
    "momemntum is like a ball in a bowl, it cares about previous gradients. It subtracts the local gradient from momentum vector at each iteration. Updates weights by adding momentum, gradient is used as an acceleration not speed\n",
    "\n",
    "To simulate some sort of friction and prevent momentum from growing large new hyperparameter β\n",
    "0 (high friction) 1 (low) good to have friction gets rid of oscillations and speeds up convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8863e4-b67d-46d0-afdd-b0664f201014",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "$$ w := w - \\alpha \\nabla J(w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ebe59-3c2f-43de-b1d1-c7bd7d4819a6",
   "metadata": {},
   "source": [
    "### Momentum Gradient Descent:\n",
    "$$ v := \\beta v - \\alpha \\nabla J(w) $$\n",
    "$$ w := w + v $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1825893c-7bf6-4726-aaaa-23d9dc5d2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8677aa-d3eb-4427-9996-1b820231d25a",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "a variant to momentum optimization measures the gradient of the cost function not local but slightly ahead. Applies the gadients after momentum step (faster than regular momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093d472-957f-4fd6-a763-82a1531de126",
   "metadata": {},
   "source": [
    "$$ v := \\beta v - \\alpha \\nabla J(w + \\beta v) $$\n",
    "$$ w := w + v $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cab8690-a1a1-46d8-9cd7-863ec0588706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343645fd-cfe9-4596-81c0-b537b2277a6f",
   "metadata": {},
   "source": [
    "### AdaGrad Adaptive Gradient Algorithm\n",
    "\n",
    "corrects direction to global optimum by scaling down gradient vecotr along steepest dimensions\n",
    "\n",
    "$$ G_t := G_{t-1} + \\nabla J(w_t)^2 $$\n",
    "$$ w_{t+1} := w_t - \\frac{\\alpha}{\\sqrt{G_t} + \\epsilon} \\nabla J(w_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c5ce7-003b-47f7-a480-6efe13127e4f",
   "metadata": {},
   "source": [
    "AdaGrad performs well for simple quadratic problems, it often stops too early when training NN: the learning rate gets scaled down so much that the algorithm ends up stopping before reaching the global optimum. Runs the risk of never converging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d9df1-2903-42ac-ae0c-f3087ace91e7",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "accumulates from most recent iterations, as opposed to all the gradients since beginning, uses exponential decay in first step:\n",
    "\n",
    "$$ E[g^2]_t := \\beta E[g^2]_{t-1} + (1 - \\beta) \\nabla J(w_t)^2 $$\n",
    "$$ w_{t+1} := w_t - \\frac{\\alpha}{\\sqrt{E[g^2]_t} + \\epsilon} \\nabla J(w_t) $$\n",
    "\n",
    "this was perferred before adam optimizer came around did much better (except on simple problems) than AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27278217-3c48-444c-a9f4-bfcfc0fdc664",
   "metadata": {},
   "source": [
    "### Adam adaptive moment estimation\n",
    "\n",
    "keeps track of exponentially decaying average of past gradients (not sum!)\n",
    "\n",
    "(the decaying average is just 1 – β1 times the decaying sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a1e27-29be-4aec-b3c9-496aa82e8073",
   "metadata": {},
   "source": [
    "#### Update biased moment estimates\n",
    "\n",
    "$$ m_t := \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla J(w_t) $$  \n",
    "$$ v_t := \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla J(w_t)^2 $$\n",
    "\n",
    "#### biased correction\n",
    "$$ \\hat{m}_t := \\frac{m_t}{1 - \\beta_1^t} $$  \n",
    "$$ \\hat{v}_t := \\frac{v_t}{1 - \\beta_2^t} $$\n",
    "\n",
    "#### updated weights\n",
    "$$ w_{t+1} := w_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3d9bf3c-0ad5-4992-a4d2-ca696d07beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbfb75-c6d8-4117-b542-3af5691af8c5",
   "metadata": {},
   "source": [
    "Three variants of Adam:\n",
    "- AdaMax\n",
    "- Nadam\n",
    "- AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5c9b5-df86-48bc-b497-4c27cf82a1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (pyenv)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
