{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d390b6a8-9be9-485b-bdaa-047b3c5c4596",
   "metadata": {},
   "source": [
    "### Exponential Linear Unit as an activation function outperformed ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23232c67-e8b9-4722-b125-b1f6421aa098",
   "metadata": {},
   "source": [
    "it takes on negative numbers allowing the training to not have vanishing gradients\n",
    "\n",
    "nonzero gradient for z < 0 so no dead neurons\n",
    "\n",
    "Helps gradient descent beacause the function is smooth everywhere (doesn't bounch as much)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b7042-55b9-4544-9c82-b66ec2b21595",
   "metadata": {},
   "source": [
    "### SELU is a scaled ELU activation function\n",
    "\n",
    "Cannot use regularization techiques like l1 or l2, max-norm, batch-norm, regular dropout\n",
    "\n",
    "self-normalizing is only guarenteed with plain MLPs\n",
    "\n",
    "input features must be standardized: mean 0 and SD of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4936916-4b8b-4d96-a9b8-0bfa1535e7f9",
   "metadata": {},
   "source": [
    "### GELU Gaussian Error Linear Units looks like ReLU but is smooth all over makes gradient descent easier to fit complex problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35eb22-112e-42d1-ac91-bc4875e780d5",
   "metadata": {},
   "source": [
    "SiLU activation outperformed GELU (Swish, β to scale sigmoid function's input)\n",
    "\n",
    "Mish is smooth, nonconvex, and nonmonotonix variant of ReLU and outperformed Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fcd6b1-4025-4880-864b-7197f7f3cd3b",
   "metadata": {},
   "source": [
    "### ReLU is a good default (hardware accelerators provide ReLU-specific optimizations)\n",
    "\n",
    "Switch is better default for more complex tasks, Mish may give slightly better results\n",
    "\n",
    "for runtime latency LeakyReLU or Parameterized Leaky ReLU for complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e4649-8b3c-43bc-86f1-b2912b79ea1a",
   "metadata": {},
   "source": [
    "### Batch Normalization (BN) reduce the danger of vanishing/exploding gradients\n",
    "\n",
    "adding an operation in model before or after activation function of each hidden layer. Zero-centers and normalizess each input, the nscales and shifts (using two new parameter vectors per layer scale and shifting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b801a1-15cc-48df-8782-77d8ad579b42",
   "metadata": {},
   "source": [
    "No need for `StandardScaler` or `Normalization` if BN is first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06964f-cfe7-47f1-bf1d-75fe46721af4",
   "metadata": {},
   "source": [
    "#### **1. Compute the Mini-Batch Mean**\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "$$\n",
    "\n",
    "#### **2. Compute the Mini-Batch Variance**\n",
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "$$\n",
    "\n",
    "#### **3. Normalize the Inputs**\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "where $ \\epsilon $ is a small constant to prevent division by zero. (smoothing term)\n",
    "\n",
    "#### **4. Scale and Shift**\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "where:\n",
    "- $ \\gamma $ (scale) and $ \\beta $ (shift) are **learnable parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674967a-65e8-4a89-9295-fc9a425fe93b",
   "metadata": {},
   "source": [
    "if we want to test predicitions for individual instances rather than batches we only have the batch mean/SD so:\n",
    "\n",
    "most implementations of batch normalization estimate final statistics during training by using a moving\n",
    "average of the layer's input means and standard deviations, Keras does this automatically.\n",
    "\n",
    "it's possible to fuse the BN layer with the previous layer after training, avoiding the runtime penalty.\n",
    "This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. \n",
    "\n",
    "previous layer computes XW + b, then the BN layer will compute γ⊗(XW + b – μ) / σ + β (ignoring the smoothing term ε). W' = γ⊗W /\n",
    "σ and b′ = γ⊗(b – μ) / σ + β, the equation simplifies to XW' + b'. replace the previous layer's weights and biases (W and b) with the updated weightsand biases (W' and b'), we can get rid of the BN layer (⊗ element-wise multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15322935-32e2-49fa-a804-9968d3f70bf2",
   "metadata": {},
   "source": [
    "For small networks might not have much impact but you can see for deeper networks this can make a huge difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a172f1a-133d-4b5f-8354-a920fb65cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631576a1-1f5c-4733-8f15-02f199dc02c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0589668-0ab0-441a-92e9-98684071e942",
   "metadata": {},
   "source": [
    "first BN layer: 3,136 parameters = 4 × 784 (γ, β, μ, and σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d02ee-349f-44b3-9771-d7c2dda591e1",
   "metadata": {},
   "source": [
    "μ and σ, are the moving averages not trainable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9861457f-8b51-4c2c-a81c-44fc7f7767ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gamma', True),\n",
       " ('beta', True),\n",
       " ('moving_mean', False),\n",
       " ('moving_variance', False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ea9449-5be1-429f-8fd7-bca0aa1d4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is some debate to put the BN before/after activation function\n",
    "# BN layer includes one offset parameter per input, you can remove the bias term from the previous layer \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbaa4dc-2e46-435c-8c06-6f4150f9b9e1",
   "metadata": {},
   "source": [
    "Hyperparams for `BatchNormalization` momentum:\n",
    "\n",
    "$\\hat{v}$: running average\n",
    "\n",
    "$\\hat{v}$ <- $\\hat{v}$ × momentum + v × (1 − momentum)\n",
    "\n",
    "axis:\n",
    "\n",
    "defaults to -1, last axis (using means and SD computed across other axes)\n",
    "if you you want to treat each pixel independently axis=[1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5850ed5-bba3-4da6-90d0-3c8ae1d7bcd3",
   "metadata": {},
   "source": [
    "### Gradient Clipping mitigates the exploding gradients (setting a threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ddfca3-1364-463f-a5cf-5037c4d7f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizer.SGD(clipvalue=1.0)\n",
    "# model.compile([...], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932ce1e-828a-4b5f-9d62-64bc5d4a2316",
   "metadata": {},
   "source": [
    "clipping should be done by setting the threshold of the norm\n",
    "usually we want to clip between -1.0 and 1.0\n",
    "\n",
    "we don't want to change the orientation of gradient example: [0.9, 100.0] to [0.9, 1.0]\n",
    "clipnorm=1.0 will do this instead [0.9, 100.0] -> [0.00899, 0.9999] keeping orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483939a1-ecf9-46b6-8790-a2b704a68c1a",
   "metadata": {},
   "source": [
    "### Resusing Layers\n",
    "\n",
    "You can re-use lower layers of a Deep NN by freezing them so backpropigation only performs on top layers instead of dropping them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753cb3d-ae4c-43c4-8c34-bc167ca6461b",
   "metadata": {},
   "source": [
    "### transfer learning\n",
    "\n",
    "say you have images of T-shirts and sandels and want to use the training from the fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c29231e-5a7f-437e-a8b4-de9e4314c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model_A was trained on fashion dataset\n",
    "# model_A = tf.keras.models.load_model(\"my_model_A\")\n",
    "# model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
    "# model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950b2611-96e1-458b-8bdf-1029c43d83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "# model_A_clone.set_weights(model_A.get_weights()) # otherwise weights are initialized randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ea9f5c-0803-4d59-80f9-53ddb42b1686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    for layer in model_B_on_A.layers[:-1]:\\n        layer.trainable = False\\n\\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\\n    model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to avoid large error gradients that may wreck the reused wweights\n",
    "# freeze the reused layers during first few epochs\n",
    "\"\"\"\n",
    "    for layer in model_B_on_A.layers[:-1]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "    model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144622d-f395-4965-b1d3-783666a222f3",
   "metadata": {},
   "source": [
    "now unfreeze reused layers and continue training, good idea to reduce learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b53e457-a25c-458e-8c4f-b832fdb5500c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhistory = model_B_on_A.fit(X_trian_B, y_train_B, epochs=4,\\n                           validation_data=(X_valid_B, y_valid_B))\\n\\nfor layer in model_B_on_A.layers[:-1]:\\n    layer.trainable = True\\n\\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\\nmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\\n                           validation_data=(X_valid_B, y_valid_B))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "history = model_B_on_A.fit(X_trian_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "676f5e9f-7d0a-4514-9228-bdba03641f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf0075-7ce9-452a-9227-8180135d7e26",
   "metadata": {},
   "source": [
    "Transfer learning only works well with Deep convolutional neural networks, not small dense networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b40f8b-2c42-41be-bb5a-c5931246372b",
   "metadata": {},
   "source": [
    "### Unsupervised pretraining\n",
    "\n",
    "you can use this if you don't have much labeled training data, and you cannot find a model trained on a similar task, could use autoencoders or GANs, and the finial task is just on the labeled data using supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b4a98-33bc-43ba-ae28-b3cdf6a76d26",
   "metadata": {},
   "source": [
    "### optimizers\n",
    "\n",
    "(Regular gradient descent will take small steps when slope is gentle and big steps when slope is steep but will never pick up speed)\n",
    "\n",
    "- momentum\n",
    "- Nesterov accelerated gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abbd34-b8a4-45e3-bdc2-6a0d8d831fa1",
   "metadata": {},
   "source": [
    "momemntum is like a ball in a bowl, it cares about previous gradients. It subtracts the local gradient from momentum vector at each iteration. Updates weights by adding momentum, gradient is used as an acceleration not speed\n",
    "\n",
    "To simulate some sort of friction and prevent momentum from growing large new hyperparameter β\n",
    "0 (high friction) 1 (low) good to have friction gets rid of oscillations and speeds up convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8863e4-b67d-46d0-afdd-b0664f201014",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "$$ w := w - \\alpha \\nabla J(w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ebe59-3c2f-43de-b1d1-c7bd7d4819a6",
   "metadata": {},
   "source": [
    "### Momentum Gradient Descent:\n",
    "$$ v := \\beta v - \\alpha \\nabla J(w) $$\n",
    "$$ w := w + v $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1825893c-7bf6-4726-aaaa-23d9dc5d2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8677aa-d3eb-4427-9996-1b820231d25a",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "a variant to momentum optimization measures the gradient of the cost function not local but slightly ahead. Applies the gadients after momentum step (faster than regular momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093d472-957f-4fd6-a763-82a1531de126",
   "metadata": {},
   "source": [
    "$$ v := \\beta v - \\alpha \\nabla J(w + \\beta v) $$\n",
    "$$ w := w + v $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cab8690-a1a1-46d8-9cd7-863ec0588706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343645fd-cfe9-4596-81c0-b537b2277a6f",
   "metadata": {},
   "source": [
    "### AdaGrad Adaptive Gradient Algorithm\n",
    "\n",
    "corrects direction to global optimum by scaling down gradient vecotr along steepest dimensions\n",
    "\n",
    "$$ G_t := G_{t-1} + \\nabla J(w_t)^2 $$\n",
    "$$ w_{t+1} := w_t - \\frac{\\alpha}{\\sqrt{G_t} + \\epsilon} \\nabla J(w_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c5ce7-003b-47f7-a480-6efe13127e4f",
   "metadata": {},
   "source": [
    "AdaGrad performs well for simple quadratic problems, it often stops too early when training NN: the learning rate gets scaled down so much that the algorithm ends up stopping before reaching the global optimum. Runs the risk of never converging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d9df1-2903-42ac-ae0c-f3087ace91e7",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "accumulates from most recent iterations, as opposed to all the gradients since beginning, uses exponential decay in first step:\n",
    "\n",
    "$$ E[g^2]_t := \\beta E[g^2]_{t-1} + (1 - \\beta) \\nabla J(w_t)^2 $$\n",
    "$$ w_{t+1} := w_t - \\frac{\\alpha}{\\sqrt{E[g^2]_t} + \\epsilon} \\nabla J(w_t) $$\n",
    "\n",
    "this was perferred before adam optimizer came around did much better (except on simple problems) than AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27278217-3c48-444c-a9f4-bfcfc0fdc664",
   "metadata": {},
   "source": [
    "### Adam adaptive moment estimation\n",
    "\n",
    "keeps track of exponentially decaying average of past gradients (not sum!)\n",
    "\n",
    "(the decaying average is just 1 – β1 times the decaying sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a1e27-29be-4aec-b3c9-496aa82e8073",
   "metadata": {},
   "source": [
    "#### Update biased moment estimates\n",
    "\n",
    "$$ m_t := \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla J(w_t) $$  \n",
    "$$ v_t := \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla J(w_t)^2 $$\n",
    "\n",
    "#### biased correction\n",
    "$$ \\hat{m}_t := \\frac{m_t}{1 - \\beta_1^t} $$  \n",
    "$$ \\hat{v}_t := \\frac{v_t}{1 - \\beta_2^t} $$\n",
    "\n",
    "#### updated weights\n",
    "$$ w_{t+1} := w_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3d9bf3c-0ad5-4992-a4d2-ca696d07beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbfb75-c6d8-4117-b542-3af5691af8c5",
   "metadata": {},
   "source": [
    "Three variants of Adam:\n",
    "- AdaMax\n",
    "- Nadam\n",
    "- AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89288f8-86c0-4025-b1e4-b8f87ca25821",
   "metadata": {},
   "source": [
    "### AdaMax\n",
    "\n",
    "works like the equations above (Adam) but replaces step 2 `s <- max(β2s, abs(∇θJ(θ)))` drops step 4 and in step 5 updates by a factor of s which is the max of the absolute value of the time-decayed gradients. Could be more stable than Adam depends on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e66e0-679a-47ef-9130-0fd06afebce7",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "\n",
    "Adam optimization + Nesterov trick, converge faster than Adam. Outperforms Adam but sometimes outperformed by RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38654b-8ed1-4f7d-a46b-a8b95a272c7b",
   "metadata": {},
   "source": [
    "### AdamW\n",
    "\n",
    "integrates a regularization technique called weight decay which reduces the size of models weights at each training interation by mutliplying them by a decay factor 0.99.\n",
    "\n",
    "Combining Adam with l2 regularization results in models that don't generalize well as those produced by SGD, AdamW fixes this issue.\n",
    "\n",
    "If none of the adam techniques work the data you might be using might not like adaptive gradients, try NAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07230f8-ab8b-4f21-863b-15b6b52dc63e",
   "metadata": {},
   "source": [
    "`tf.keras.optimizers.Adam` `tf.keras.optimizers.Nadam` `tf.keras.optimizers.Adamax` `tf.keras.optimizers.experimental.AdamW` with AdamW we may want to tune the `weight_decay` hyperparam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeee5a0-e8d0-4723-a199-ced99b45f895",
   "metadata": {},
   "source": [
    "These optimizers are only first-order partial derivatives (Jacobians). Hessians are hard to compute in NN and are slow they don't even fit in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023f963-6b89-47dd-965c-ce3dde4c95c6",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "setting too high may diverge, too low may take a very long time to converge to optimum. Its good to start with large learning rate then reduce it.\n",
    "\n",
    "It can also be beneficial to start with a low learning rate, increase it, then drop it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431e236-f806-464f-9bb1-19fbcd859eae",
   "metadata": {},
   "source": [
    "#### Scheduling\n",
    "- Power:\n",
    "  η(t) = η0 / (1 + t/s)c drops at each step: n/2 -> n/3 -> n/4 and so on\n",
    "- Exponential:\n",
    "  η(t) = η0 0.1t/s gradually drop by a factor of 10 every steps\n",
    "- Piecewise constant:\n",
    "  n0 = 0.1 for 5 epochs, n1 = 0.001 for 50 epochs, involves fiddling aronud\n",
    "- Performance:\n",
    "  Measure the validation error every N steps, reduce the learning rate by a factor of λ\n",
    "- 1cycle:\n",
    "  starts by increasing initial learning rate till n1 (halfway through training) then decreasing learning rate back down to n0\n",
    "\n",
    "both performance and exponential scheduling performs well, but 1cycle might perform even better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aac34e90-7edd-4e98-b540-1312a70212ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power scheduling\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)\n",
    "# ^ old way of doing this\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,  # Number of steps for the decay to happen\n",
    "    end_learning_rate=0.0001,  # Final learning rate after decay\n",
    "    power=1.0  # Power for polynomial decay (set to 1.0 for linear decay)\n",
    ")\n",
    "\n",
    "# Set up the optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf1e13c5-0983-4061-9679-eaa77751a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential decay\n",
    "def exponential_decay_fn(epoch):\n",
    "    lr0 = 0.01  # Initial learning rate\n",
    "    s = 20  # Decay step\n",
    "    return lr0 * 0.1 ** (epoch / s)\n",
    "\n",
    "# Define the LearningRateScheduler callback with the decay function\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc1113a9-3d73-4631-a5fa-94eff750ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theres a built-in exponential decay:\n",
    "# Define the exponential decay schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,  # Initial learning rate\n",
    "    decay_steps=1000,  # Number of steps per decay\n",
    "    decay_rate=0.96,  # Decay factor\n",
    "    staircase=True,  # If True, decay in discrete intervals (step-wise)\n",
    ")\n",
    "\n",
    "# Use the learning rate schedule with an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e95afd-a826-400c-b227-75c4b73955cd",
   "metadata": {},
   "source": [
    "### Exponential Decay Example\n",
    "\n",
    "Let’s assume the following parameters for the exponential decay:\n",
    "\n",
    "- Initial learning rate \\( \\text{lr}_0 = 0.01 \\)\n",
    "- Decay rate \\( \\text{decay\\_rate} = 0.96 \\)\n",
    "- Decay steps \\( \\text{decay\\_steps} = 1000 \\)\n",
    "\n",
    "The formula for exponential decay is:\n",
    "\n",
    "$$\n",
    "\\text{lr}(t) = \\text{lr}_0 \\times \\text{decay\\_rate}^{\\frac{t}{\\text{decay\\_steps}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\text{lr}_0 \\) is the initial learning rate.\n",
    "- \\( t \\) is the current step (epoch).\n",
    "- \\( \\text{decay\\_rate} \\) is the factor by which the learning rate is reduced at each step.\n",
    "- \\( \\text{decay\\_steps} \\) is the number of steps per decay.\n",
    "\n",
    "#### Step 1: Initial Learning Rate\n",
    "\n",
    "At the start (epoch 0), the learning rate is:\n",
    "\n",
    "$$\n",
    "\\text{lr}(0) = 0.01\n",
    "$$\n",
    "\n",
    "#### Step 2: After 1 Step (Epoch 1)\n",
    "\n",
    "After 1 step, the learning rate decays by the `decay_rate` of 0.96. The new learning rate will be:\n",
    "\n",
    "$$\n",
    "\\text{lr}(1) = 0.01 \\times 0.96 = 0.0096\n",
    "$$\n",
    "\n",
    "#### Step 3: After 2 Steps (Epoch 2)\n",
    "\n",
    "After 2 steps, the learning rate will decay further, and the new learning rate is:\n",
    "\n",
    "$$\n",
    "\\text{lr}(2) = 0.01 \\times 0.96^2 = 0.01 \\times 0.9216 = 0.009216\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8558ac-96d7-48dd-8422-14329b1e8f1a",
   "metadata": {},
   "source": [
    "You could have the learning rate update at every steps (if you have many) instead of at the beginning of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b22a49b1-546e-45f0-ae1c-0a1095535676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# piece-wise\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "# performance\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "# history = model.fit(X_trian, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56daec68-6920-4811-8452-d76c4960543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates after each step:\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Generate synthetic training data (1000 samples, 10 features each)\n",
    "X_train = np.random.rand(1000, 10).astype(np.float32)\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 25\n",
    "n_steps = n_epochs * math.ceil(len(X_train) / batch_size)\n",
    "scheduled_learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01, decay_steps=n_steps, decay_rate=0.1)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=scheduled_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727d9d8-0921-49b4-9fe6-a21d1ad36271",
   "metadata": {},
   "source": [
    "### L1 and L2 Regularization (L1: Lasso Regression, L2: Ridge Regression)\n",
    "\n",
    "L2 to contrain a NN connection weights, L1 if you want a sparse model)\n",
    "\n",
    "L1 penalty term: adds the sum of absolute values of the weights to loss function\n",
    "\n",
    "L1 feature selection: selectes important features\n",
    "\n",
    "L2 penalty term: Adds the sum of the squared values of the weights to the loss function\n",
    "\n",
    "L2 feature selections: all features contribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d98b7395-ef4f-4317-8206-31329b0615f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0ed30-5118-49b3-985e-10befd08cae8",
   "metadata": {},
   "source": [
    "if both are needed: `tf.keras.regularizers.l1_l2()` L1 + L2 = Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd54015-e4c7-4b28-8a61-0bbeac44f8a8",
   "metadata": {},
   "source": [
    "$$\n",
    "L1\\_loss = \\lambda \\sum |w_i|\n",
    "$$\n",
    "\n",
    "$$\n",
    "L2\\_loss = \\lambda \\sum w_i^2\n",
    "$$\n",
    "\n",
    "Elastic Net combines both penalties:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\sum |w_i| + \\lambda_2 \\sum w_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2e4bb-ab89-4c14-8bdd-14d8708f56aa",
   "metadata": {},
   "source": [
    "instead of repeating arguments (same regularizer, activation, initialization) in all hidden layers, you could use loops, but there is `functools.partial()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db068843-0d76-40a9-891b-4793b0971cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb0d03-4f56-4bc0-a6c0-3efa00e144bb",
   "metadata": {},
   "source": [
    "Do not use Adam with L2 use AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e131e-5c0e-4a6c-8fd8-62deccfc99b6",
   "metadata": {},
   "source": [
    "### Dropout Regularization\n",
    "\n",
    "at every training step it ignores neurons\n",
    "\n",
    "improves accuracy 1-2%\n",
    "\n",
    "dropout rate hyperparam is p set between 10-50%, 20-30% in recurrent NN, 40-50% CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c9b2e88-e556-4af0-9ad1-f91fd0d79309",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28,28]),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f00e42-728b-46dd-9121-00628a376f0b",
   "metadata": {},
   "source": [
    "make sure to evaluate the training loss without dropout (after training)\n",
    "\n",
    "increase dropout if overfitting and vise-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709beb4-8828-4bd2-9af0-6aa353cb9cf5",
   "metadata": {},
   "source": [
    "### Monte Carlo (MC) Dropout\n",
    "\n",
    "can improve performance of any trained dropout model without having to retrain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97608b85-19ca-4e11-a647-d7afad75098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data (1000 samples, 10 features each)\n",
    "X_test = np.random.rand(200, 28, 28).astype(np.float32)\n",
    "\n",
    "y_probas = np.stack([model(X_test, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fb263-f5e8-4cc7-ad70-3af690757779",
   "metadata": {},
   "source": [
    "model(X) is similar to model.predict(X) except it returns a tensor rather than a NumPy array\n",
    "\n",
    "training=True ensures that the Dropout layer remains active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a837317-b69a-4636-a522-d61f86af04ed",
   "metadata": {},
   "source": [
    "Averaging over multiple predictions with dropout turned on gives us a Monte Carlo estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb838f8-ea71-43a7-b25c-3fd8d4c0d632",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization\n",
    "\n",
    "explicitly limits the magnitude of the weight vectors by constraining them within a fixed norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772913d2-4eaf-4175-a94b-1816a49ffd8c",
   "metadata": {},
   "source": [
    "reducing r helps reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6297ea8-26c7-4e2c-8752-4c08508c5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_constraint=tf.keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4934df5c-945d-49a0-97a3-05449d2cfe96",
   "metadata": {},
   "source": [
    "If a sparse model is needed use L1 regularization zero out tiny weights, if a sparser model is needed use TensorFlow Model Optimization Toolkit\n",
    "\n",
    "low-latency model performs lightning-fast predictions, use ReLU or leaky RelU fold bath normalization layers into previous layers, reduce float precision from 32 bits to 16 or 8\n",
    "\n",
    "use MC dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5c4d0-22d1-49b0-8fd9-5afd7550a92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (pyenv)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
