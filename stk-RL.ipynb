{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066aec1b-cd49-47f1-8a72-a7c1387d8263",
   "metadata": {},
   "source": [
    "### OpenAI Gym\n",
    "\n",
    "A toolkit that provides a wide variety of simulated environments used to train agents, compare them, or develop new RL Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c7222d-9081-4557-90f9-b68035a044dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27bd575-c9a0-4bf4-8ec0-e560102974e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (25.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (75.8.0)\n",
      "Requirement already satisfied: wheel in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.45.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb09eb-64df-4173-ab16-ad657aa26264",
   "metadata": {},
   "source": [
    "(had to downgrade python env to 3.10 for the below installations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50e34a8-3378-4735-9e48-976d4463d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \"gym[classic_control,box2d,atari,accept-rom-license]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d84f26-f08b-4361-a734-b4fe27ae843c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box2d-py in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"box2d-py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33050b79-55e8-44ff-80d5-d991200c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec70793-6105-4778-ab0a-37b37b10f931",
   "metadata": {},
   "source": [
    "A cartpole is a pole on a moving cart that when the cart is pushed the poll is falling down at an angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67bb9b-a835-4e26-a6de-54ed1635026d",
   "metadata": {},
   "source": [
    "A cartPole environment returns 1D NumPy array containing four floats representing the cart’s horizontal position (0.0 = center), its velocity (positive means right), the angle of the pole (0.0 = vertical), and its angular velocity (positive means clockwise). `reset()` returns a dictionary that may contain extra environment-specific information Atari environments, it contains the number of lives left, in CartPole its empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a99db5cb-4551-4313-80c2-292068addb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1416a279-866b-423e-b711-6ad3bc6a6baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92dfa518-fef2-4e0c-bd90-a7f2ebc35431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46d19c9c-ed17-4c91-8221-ea386d5c1d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94786548-9032-43d0-a546-3fb035319965",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fabc5224-c67c-475d-beb4-713dd2a7231c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Turn off the axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b6fa961-9464-4331-b1fc-f9a5f40c8574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # 2 meaning left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd4cd8a3-00e0-4540-88e7-a591c04d1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1 # accelerate right\n",
    "obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d8dde0d-045e-414c-9ced-82ddcbbbe14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03870419,  0.5777363 ,  0.02017712, -0.8251987 ], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17e0919b-53fa-45ab-8f87-d2ff836d1870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9d1b599-0d10-4072-8243-8f26581f7331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d142ee4c-a162-4b1b-890c-16c225be4666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22f7e88a-34fa-49bb-8814-832df52a1567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1216eb7-d43b-4444-a07c-5832b2ddd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ac18a4c-c7fb-4f8a-9afb-f1828397dfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(41.698), np.float64(8.389445512070509), 24.0, 63.0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(totals), np.std(totals), min(totals), max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb451a6-21c8-4800-bbf7-1a09e41b880f",
   "metadata": {},
   "source": [
    "With 500 tries this policy never managed to keep the pole upright for more than 63 consecutive steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e6ced-795c-49e8-ab19-522cd15591dd",
   "metadata": {},
   "source": [
    "exploration/exploitation dilemma is central in reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d669a2c4-308a-409d-9209-fc3a31255d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.0.2-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.12.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.14.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mathias/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp310-cp310-macosx_12_0_arm64.whl (239.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp310-cp310-macosx_12_0_universal2.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp310-cp310-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp310-cp310-macosx_10_9_universal2.whl (397 kB)\n",
      "Downloading numpy-2.0.2-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp310-cp310-macosx_11_0_arm64.whl (324 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, grpcio, google-pasta, gast, astunparse, absl-py, werkzeug, ml-dtypes, markdown-it-py, h5py, tensorboard, rich, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.12.1 keras-3.8.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.14.0 protobuf-5.29.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "476e2305-f1a5-4a6f-b869-7b98fd196ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"), # prob of going left\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7abc7-7cde-4fea-835b-f0f4237aaf98",
   "metadata": {},
   "source": [
    "If there were more than two possible actions, there would be one output neuron per action, and we would use the softmax activation function instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e6ed1-90a3-41ad-8536-8b140eead2a1",
   "metadata": {},
   "source": [
    "credit assignment problem: when the agent gets a reward, it is hard for it to know which actions should get credited (or blamed) for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b79e2f-37bb-4448-9b96-aeba49b457d9",
   "metadata": {},
   "source": [
    "### Policy Gradients\n",
    "\n",
    "PG algorithms optimize the parameters of a policy by following the gradients toward higher rewards\n",
    "\n",
    "we will pretend for now that whatever action it takes is the right one so we can compute the loss and its gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d9c21f1-5fe5-4fd3-bccb-770def7cb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "526731d3-927c-4d54-b164-a8fa57b24483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode  in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(\n",
    "                env, obs, model, loss_fn\n",
    "            )\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254bdf9-1832-495e-860e-c94307a3ec6e",
   "metadata": {},
   "source": [
    "compute the sum of future discounted rewards at each step, then normalize all these discounted rewards (the returns) across many episodes by subtracting the mean and dividing by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7ba4491-e46e-4a59-a9a5-19683268878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "adaf9c4e-66ec-4f3f-a32f-b5335cea3e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6de452be-6436-401e-83dd-890dca4457b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c002a078-20d3-4c77-bd90-3444e7e71cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e311ecaa-6012-439a-ad6e-6dbe2ca4ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c686df-144c-4612-b320-73f95b6456d0",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b6926-ec2d-4661-aec4-0dbd5341cec2",
   "metadata": {},
   "source": [
    "# Understanding Discounted Rewards in Reinforcement Learning\n",
    "\n",
    "## Key Concepts:\n",
    "- **Rewards**: Numerical values the agent receives after performing an action. A higher reward is generally better.\n",
    "- **Discount Factor (γ, gamma)**: A value between 0 and 1 that determines the importance of future rewards compared to immediate rewards.\n",
    "  - If **γ is close to 0**, the agent prioritizes **immediate rewards** and largely ignores future rewards.\n",
    "  - If **γ is close to 1**, the agent considers **future rewards almost as important** as immediate rewards.\n",
    "  \n",
    "- **Return**: The total amount of reward an agent expects to accumulate over time, starting from a certain state and action. It is the sum of all future rewards, adjusted by the discount factor.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Example:\n",
    "In the given example, the agent takes three actions and receives the following rewards:\n",
    "1. **First action**: +10 reward\n",
    "2. **Second action**: 0 reward\n",
    "3. **Third action**: –50 reward  \n",
    "\n",
    "Let’s assume the discount factor **γ = 0.8**.\n",
    "\n",
    "The **return** for the first action is calculated as the sum of all future rewards (considering the discount factor):\n",
    "\n",
    "1. Immediate reward:  \n",
    "   \\[\n",
    "   +10\n",
    "   \\]\n",
    "   \n",
    "2. Second action’s reward is 0, discounted by γ:  \n",
    "   \\[\n",
    "   γ $\\times$ 0 = 0.8 $\\times$ 0 = 0\n",
    "   \\]\n",
    "\n",
    "3. Third action’s reward is –50, discounted by γ²:  \n",
    "   \\[\n",
    "   γ² $\\times$ (-50) = 0.8² $\\times$ (-50) = 0.64 $\\times$ (-50) = -32\n",
    "   \\]\n",
    "\n",
    "Summing these:\n",
    "\n",
    "\\[\n",
    "\\text{Return} = 10 + 0 + (-32) = -22\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition:\n",
    "- **When γ is closer to 1** (e.g., 0.99), the agent considers future rewards nearly as important as immediate rewards.\n",
    "- **When γ is closer to 0** (e.g., 0.1), the agent mostly cares about immediate rewards, and future rewards have little impact.\n",
    "\n",
    "### Typical Discount Factors:\n",
    "- **γ = 0.9 to 0.99** is common in RL problems.\n",
    "- A γ of **0.95** means that rewards **13 steps into the future** count for about half as much as immediate rewards.\n",
    "- A γ of **0.99** means that rewards **69 steps into the future** count for about half as much.\n",
    "\n",
    "---\n",
    "\n",
    "## CartPole Environment:\n",
    "In **CartPole**, where the goal is to balance a pole on a cart, the effects of actions are **short-term**.  \n",
    "A **discount factor of 0.95** is reasonable because the agent needs to focus more on the immediate effects of its actions to keep the pole balanced.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Does This Matter?\n",
    "By adjusting **γ**, we control how much the agent \"cares\" about the future:\n",
    "- **Higher γ (0.99)** → More long-term planning.\n",
    "- **Lower γ (0.9 - 0.95)** → More short-term decision-making.\n",
    "\n",
    "If the problem requires **long-term planning** (like playing chess), a **higher γ** is better.  \n",
    "For problems with **short-term goals** (like balancing a CartPole), a **lower γ** works well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb72a5f2-e75f-4e9c-8756-de6eb4d9f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn\n",
    "    )\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0\n",
    "            )\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f3911-6fe4-4942-8551-26d92b24edde",
   "metadata": {},
   "source": [
    "## **Q-Value Iteration Algorithm Equation**  \n",
    "The Q-value iteration updates the Q-values iteratively using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow \\mathbb{E} \\left[ r + \\gamma \\max_{a'} Q(s', a') \\mid s, a \\right]\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\( Q(s, a) \\) is the Q-value for state \\( s \\) and action \\( a \\).  \n",
    "- \\( r \\) is the immediate reward received.  \n",
    "- \\( \\gamma \\) is the discount factor.  \n",
    "- \\( s' \\) is the next state.  \n",
    "- \\( a' \\) is the next action.  \n",
    "- \\( \\max_{a'} Q(s', a') \\) ensures we take the best possible future action.\n",
    "\n",
    "---\n",
    "\n",
    "## **Value Iteration Algorithm**  \n",
    "Value iteration updates the value function iteratively:\n",
    "\n",
    "$$\n",
    "V(s) \\leftarrow \\max_{a} \\mathbb{E} \\left[ r + \\gamma V(s') \\mid s, a \\right]\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\( V(s) \\) is the value of state \\( s \\), representing the maximum expected return from \\( s \\).  \n",
    "- The expectation is taken over possible next states \\( s' \\) given \\( s, a \\).  \n",
    "- The policy chooses the action \\( a \\) that maximizes future rewards.\n",
    "\n",
    "---\n",
    "\n",
    "## **Bellman Optimality Equation**  \n",
    "The Bellman optimality equation expresses the recursive relationship of the optimal value function:\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a} \\mathbb{E} \\left[ r + \\gamma V^*(s') \\mid s, a \\right]\n",
    "$$\n",
    "\n",
    "where \\( V^*(s) \\) represents the optimal value function.\n",
    "\n",
    "For Q-values, the optimal Q-function follows:\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\mathbb{E} \\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **What is \\( \\pi^*(a) \\)?**  \n",
    "\\( \\pi^*(a) \\) represents the optimal policy, which selects the best action in a given state:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_{a} Q^*(s, a)\n",
    "$$\n",
    "\n",
    "This means that the optimal policy selects the action \\( a \\) that maximizes the Q-value.\n",
    "\n",
    "---\n",
    "\n",
    "## **How They Relate and Key Differences**  \n",
    "- **Bellman optimality equation** defines the best possible value function.  \n",
    "- **Value iteration** updates values based on Bellman’s equation until convergence.  \n",
    "- **Q-value iteration** updates Q-values directly instead of state values.  \n",
    "- **\\( \\pi^*(s) \\)** derives from \\( Q^*(s, a) \\) by choosing the best action.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Example: 3-State MDP**\n",
    "Consider a simple Markov Decision Process (MDP) with three states \\( s_1, s_2, s_3 \\) and two actions \\( a_1, a_2 \\).  \n",
    "Let’s assume the following rewards and transitions:  \n",
    "- Taking action \\( a_1 \\) in \\( s_1 \\) moves to \\( s_2 \\) with a reward of \\( +5 \\).  \n",
    "- Taking action \\( a_2 \\) in \\( s_2 \\) moves to \\( s_3 \\) with a reward of \\( +10 \\).  \n",
    "- \\( s_3 \\) is a terminal state with zero reward.  \n",
    "- The discount factor \\( \\gamma = 0.9 \\).\n",
    "\n",
    "### **1. Bellman Optimality Equation**  \n",
    "Using the Bellman equation, we start with initial values \\( V(s) = 0 \\) and iteratively compute:\n",
    "\n",
    "$$\n",
    "V(s_2) = \\max(5 + 0.9 V(s_3), 0) = 5\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(s_1) = \\max(0 + 0.9 V(s_2), 0) = 4.5\n",
    "$$\n",
    "\n",
    "### **2. Value Iteration**  \n",
    "Using value iteration, we compute:\n",
    "\n",
    "$$\n",
    "V(s) \\leftarrow \\max_{a} \\mathbb{E} [r + \\gamma V(s')]\n",
    "$$\n",
    "\n",
    "until convergence.\n",
    "\n",
    "### **3. Q-Value Iteration**  \n",
    "Instead of updating \\( V(s) \\), we update \\( Q(s, a) \\):\n",
    "\n",
    "$$\n",
    "Q(s_1, a_1) = 5 + 0.9 \\max(Q(s_2, a_2), 0) = 5 + 0.9 \\times 10 = 14\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s_2, a_2) = 10 + 0.9 \\times 0 = 10\n",
    "$$\n",
    "\n",
    "### **4. Deriving the Optimal Policy \\( \\pi^* \\)**  \n",
    "The optimal policy selects the action with the highest Q-value:\n",
    "\n",
    "$$\n",
    "\\pi^*(s_1) = \\arg\\max_{a} Q^*(s_1, a) = a_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi^*(s_2) = \\arg\\max_{a} Q^*(s_2, a) = a_2\n",
    "$$\n",
    "\n",
    "Thus, the optimal strategy is:\n",
    "- Take \\( a_1 \\) in \\( s_1 \\), then take \\( a_2 \\) in \\( s_2 \\), reaching \\( s_3 \\) with maximum rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b253ca-8172-4ef9-993a-db3ff4be6f8c",
   "metadata": {},
   "source": [
    "A MDP manuallly written:\n",
    "\n",
    "from going from s2 -> s0 after playing action a1 we lookup `transition_probabilities[2][1][0]` = 0.8\n",
    "\n",
    "to look up corresponding reward we look up `rewards[2][1]`\n",
    "\n",
    "look up actions in s2: `possible_actions[2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "084287d9-0bc7-497c-a2d0-113cea7a5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "    [None, [0.8, 0.1, 0.1], None]\n",
    "]\n",
    "rewards = [\n",
    "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
    "]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "360a5698-75f5-4505-a174-04a88083b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0 # all possible actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97587083-ce15-4e65-9ec0-b2c269a04283",
   "metadata": {},
   "source": [
    "Q-value iteration algorithm (every state and every possible action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f967436-4468-4f45-bc27-0c28a4d9c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.90\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
    "                for sp in range(3)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94ab818a-b403-4978-90af-c7a5694100c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5dc3c-349e-4500-b359-66f3fbb6c8bb",
   "metadata": {},
   "source": [
    "^ when the agent is in state s0 and it chooses action a1, the expected sum of discounted future rewards is approximately 17.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ed085-ea65-41eb-ba28-a97d974c1bb2",
   "metadata": {},
   "source": [
    "For each state, we can find the action that has the highest Q-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e01588f-5675-44f5-a4ab-b11d1d3bce5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values.argmax(axis=1) # optimal action for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9414a56-55f8-48da-b23c-5a27e57f43e9",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "an adaptation of the Q-value iteration algorithm, it watches the agent play randomly and gradually improving its estimates of the Q-values (Imagine learning to play baseball when your coach is a blindfolded ape.\n",
    "\n",
    "here we start creating an agent to explore the enviornment (a step function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c130e5e-47c1-4825-9d55-58e9c38b5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c369fc3c-d3b3-4fcb-a9d1-d315037285ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e6081-4f1e-4bb1-be98-37732af669b0",
   "metadata": {},
   "source": [
    "init Q-values and run the Q-learning algorithm with learning rate decay (power scheduling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9629c2f8-8394-41e3-8f42-47bd6cb1d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0 = 0.05\n",
    "decay = 0.005\n",
    "gamma = 0.90\n",
    "state = 0\n",
    "\n",
    "for iteration in range(10_000):\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = Q_values[next_state].max()\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4ac80-c896-4352-a147-6b72833d196d",
   "metadata": {},
   "source": [
    "### Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "79032c36-2b72-4424-8d0f-9574c278ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [4] # env.observation_space.shape\n",
    "n_outputs = 2 # env.action_space.n\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf591b2-7da9-48c2-aa39-ab754dc035cc",
   "metadata": {},
   "source": [
    "ε-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1f95f441-866b-47a8-9cb2-e9ff98ba1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
    "        return Q_values.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9435a04a-62db-44a4-a8d9-70d605155f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer\n",
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc92e82e-48cc-4d12-a6bf-a69aa1f4d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    return [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(6)\n",
    "    ] # [states, actions, rewards, next_states, dones, truncates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "49fae8f7-9317-4955-9e66-896773fd70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "    return next_state, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7ce6b-9319-49b7-93dd-92b0de437b71",
   "metadata": {},
   "source": [
    "sample a batch of experiences from the replay buffer and train the DQN by performing a single gradient descent step on this batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "72b6959c-961e-494a-b37e-cde90483dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    next_Q_values = model.predict(next_states, verbose=0)\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    runs = 1.0 - (dones | truncateds) # episode is not done or truncated\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ee116b06-637a-4cb5-aa11-d3e3365396b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model:\n",
    "for episode in range(600):\n",
    "    obs, info = env.reset()\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff29e9b-a931-412f-b14d-052ac0e27c98",
   "metadata": {},
   "source": [
    "^ We run 600 episodes, each for a maximum of 200 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230de94b-701a-4fed-a15c-f3c521d5c034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
