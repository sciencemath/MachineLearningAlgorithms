{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f137c0c7-ee70-4c23-b40d-36ac28caa4f8",
   "metadata": {},
   "source": [
    "### Tensors flow from operation to operation (hence the name Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96936b38-acbb-4f3f-af93-a974728fbeb9",
   "metadata": {},
   "source": [
    "similar to Numpy ndarray, a tensor can also hold a scalar.\n",
    "\n",
    "Tensors allows for a more custom solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a154cc-3d5e-4857-b654-537f5ddbdd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t = tf.constant([[1.,2.,3.], [4.,5.,6.]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85740362-92ba-474d-986c-309c9b48dd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab711dc-3414-4891-9630-34b4cb6c46e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f2585a-5484-47dd-8827-7dcf57baf27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb19ae4-6927-43a6-82a6-bdcb097c915a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46570b-a9b4-4bac-928e-245031495d0d",
   "metadata": {},
   "source": [
    "Use `tf.reshape()` when you need to rearrange elements without adding or removing data.\n",
    "\n",
    "Use `tf.newaxis` when you need to expand dimensions (e.g., add batch/channel axes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb07f3c-b36e-4855-9560-fe7fe5e7df80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6311428-a129-4318-a8d4-861b4cb92d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8918940e-283e-47b4-bdaf-492085a82ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4677e148-2ea4-43e1-a50d-75213dc03e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=69>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb44176-4a14-4af3-9573-5876d97b22da",
   "metadata": {},
   "source": [
    "numpy transpose `t.T` is different than `tf.transpose(t)` numpy modifies the same data and tensorflow returns a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f50f01eb-95c4-422d-b647-8a01d80178f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([2.,4.,5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7024c383-2d0b-4627-89cc-1f9bf95216d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a2288d3-54ad-4e18-981e-096e948cdcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e60cd2d-ca5d-473d-8b7e-ece9da7dfd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc5a13-988a-4728-91f3-2e0018f0a2ff",
   "metadata": {},
   "source": [
    "NumPy uses 64-bit precision by default, while TensorFlow uses 32-bit. 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less\n",
    "RAM. When you create a tensor from a NumPy array, make sure to set dtype=tf.float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ccdbea3-c1f4-4615-be62-0f66027f473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These throws errors because we cannot convert datatyes\n",
    "# tf.constant(2.) + tf.constant(40)\n",
    "# tf.constant(2.) + tf.constant(40, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a70c4a06-ea83-480d-87f8-ef1ee207e448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d3b9020-1c65-4120-8b3a-cb93238b625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1.,2.,3.], [4.,5.,6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be25cb4d-f2f1-4f10-82af-f2dffa3a3585",
   "metadata": {},
   "source": [
    "TensorFlow Variables allows us to modify operations in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e4d92c2-36a9-4446-9639-e369c2571793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98d1a555-331d-41ff-abf9-b220e9231c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 69.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "055d9740-bb33-4175-8839-3edb3ae2d66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 69.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98e1e538-5b1e-44c9-8730-ec5ffc2f97fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  69.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0,0], [1,2]], updates=[100., 200.]) # updating slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d91ff355-2d2a-4ad1-aa54-c2f0f6b0e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct assignment won't work\n",
    "# v[1] = [7.,8.,9.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c049d74-49e8-4424-bdd4-82a306024bfe",
   "metadata": {},
   "source": [
    "### Custom Loss function\n",
    "\n",
    "used when you need something custom say when your data is noisy and MSE is not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b749e-fe46-48e4-b01d-709c6e1f23c8",
   "metadata": {},
   "source": [
    "`tf.keras.losses.Huber` is built in but here we can define our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac87fdeb-ab02-4c50-88fe-8873ddcdd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f32cf-f5e2-4d9f-9e56-3f7d7ad7e7b2",
   "metadata": {},
   "source": [
    "its not recommended to return the mean loss as it makes it impossible to use class weights or sample weights when you need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8767b120-0410-450d-9999-4fdf12db5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "# model.fit(X_train, y_train, [...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d385a851-4178-44e4-8830-6000cf121257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model containing custom objects\n",
    "# model = tf.keras.models.load_model(\"my_model_with_a_custom_loss\", custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3db93-6eb3-474c-a426-74fd08954fe3",
   "metadata": {},
   "source": [
    "any error between -1 and 1 is considered small, if we want a different threshold we can create a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8621f86d-cfcb-4603-91f2-2f66492e0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "# model.compile(loss=create_huber(2.0), optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a92f84f-f036-4e4d-b07c-b0296144b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model with custom threshold\n",
    "# model = tf.keras.models.load_model(\"my_model_loss_threshold_2\", custom_objects={\"huber_fn\": create_huber(2.0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e44d7-d3e1-442d-9caa-f544efd9c66f",
   "metadata": {},
   "source": [
    "when saving a model Keras calls the loss instance `get_config()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4a62f48-6d3a-4b71-b9ec-a78ea4896cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
    "# model = tf.keras.models.load_model(\"model_custom_loss_class\", custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53ab4c-7ab8-4dcb-b6d7-efc01e12c41c",
   "metadata": {},
   "source": [
    "### Custom activations, initializers, regularizers, and contraints\n",
    "\n",
    "- activation\n",
    "   softplus: equivalent to `tf.keras.acitivations.softplus()` or `tf.nn.softplus()`\n",
    "- initializer\n",
    "  glorot: equivalent to `tf.keras.initializers.glorot_normal()`\n",
    "- regularizer\n",
    "  L1: `tf.keras.regularizers.l1(0.01))`\n",
    "- constraints\n",
    "  weights: `tf.keras.contraints.nonneg()` or `tf.nn.relu()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad0a7d00-1c6f-4c44-bf28-ae9f6ebaf445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fec9ead0-3e64-4196-98ff-387502fb1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                              kernel_initializer=my_glorot_initializer,\n",
    "                              kernel_regularizer=my_l1_regularizer,\n",
    "                              kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398459f-cad0-4fe8-9a49-a910bd79a1ac",
   "metadata": {},
   "source": [
    "### hyperparams saved with model\n",
    "\n",
    "subclass correctly from `Initializer, Layer, Constraint, Regulatizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2711b51a-8b0b-40f7-91c7-8c51d83337a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "818d9859-6cb6-4673-b436-aae64eca577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom metric:\n",
    "# model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595bd54-bba9-4f45-a02f-97bba8ad7f4d",
   "metadata": {},
   "source": [
    "### Binary classifier precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45da41-f95e-4ef8-89e2-316a5830d416",
   "metadata": {},
   "source": [
    "1. **First batch:**\n",
    "   - **True Positives (TP)** = 4  \n",
    "   - **False Positives (FP)** = 1  \n",
    "   - **Precision** =  \n",
    "     $$ \\frac{TP}{TP + FP} = \\frac{4}{5} = 80\\% $$\n",
    "\n",
    "2. **Second batch:**\n",
    "   - **True Positives (TP)** = 0  \n",
    "   - **False Positives (FP)** = 3  \n",
    "   - **Precision** =  \n",
    "     $$ \\frac{0}{3} = 0\\% $$\n",
    "\n",
    "### Why the Mean is Wrong (Macro Averaging)\n",
    "If you simply average the two batch precisions:\n",
    "\n",
    "$$\n",
    "\\frac{80\\% + 0\\%}{2} = 40\\%\n",
    "$$\n",
    "\n",
    "This is **macro averaging**, which treats both batches equally regardless of the number of predictions in each.\n",
    "\n",
    "### Why 50% is Correct (Micro Averaging)\n",
    "The correct way to calculate **overall precision** is to sum up all **true positives** and divide by all **positive predictions** across both batches:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Total True Positives}}{\\text{Total Positive Predictions}} = \\frac{4 + 0}{5 + 3} = \\frac{4}{8} = 50\\%\n",
    "$$\n",
    "\n",
    "This is **micro averaging**, which takes into account the actual number of predictions.\n",
    "\n",
    "### Key Takeaway\n",
    "- **Macro averaging** (simple mean) can be misleading because it gives equal weight to each batch, regardless of size.\n",
    "- **Micro averaging** (overall TP / overall predictions) is usually the right way to calculate precision when looking at the modelâ€™s actual performance.\n",
    "\n",
    "That's why the correct answer is **50%**, not **40%**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c2db46e-5c51-4401-a99a-6a95c21941e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.800000011920929>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Micro averaging\n",
    "precision = tf.keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b66b3bb-3a10-4069-b1fe-04f4905560c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5bd31-4513-4716-9bbf-137ac989f953",
   "metadata": {},
   "source": [
    "^ This is overall percision not just the second batch! (streaming metric, stateful metric) updated batch after batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e775b7fe-7e39-4ad9-8569-e5545fcc1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c77aaca7-6956-47eb-a419-682948d8e43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Variable path=precision/true_positives, shape=(1,), dtype=float32, value=[4.]>,\n",
       " <Variable path=precision/false_positives, shape=(1,), dtype=float32, value=[4.]>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11550657-417a-49f5-883a-1be1cd2cf2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98c40e03-f35a-414d-880e-0786525cbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom streaming metric\n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        sample_metrics = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(sample_metrics))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86171ebc-0709-46aa-a3b7-f159c69a44d3",
   "metadata": {},
   "source": [
    "^ Keras will take care of variable persistence no action is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c1456-591b-4295-9220-06cc4a61b0e0",
   "metadata": {},
   "source": [
    "#### layers with no weights `tf.keras.layers.Flatten`\n",
    "#### layers with no weights `tf.keras.layers.ReLU`\n",
    "\n",
    "custom layer without weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd4aeae0-c5a0-4989-aa55-4f19c096ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation=\"exponential\"\n",
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b1117f8-a667-46a5-bee8-033bb4b9ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stateful layer (simplified Dense layer)\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "                                      initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units, \"activation\": tf.keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e2dc330-acf3-4517-b9a4-dc3b9cec44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that adds Gaussian noise during trianing but nothing for testing\n",
    "# much like tf.keras.layers.GaussianNoise\n",
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=False):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bf91d-9d96-42f6-9786-2c50cfe7265e",
   "metadata": {},
   "source": [
    "Custom Model:\n",
    "\n",
    "creating identical blocks, its more effecient to create the layers needed for the model separately. Now we have the tools to create any model we need w/ Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7cbe041-cae1-4deb-a7e9-0e8e132fc4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(\n",
    "            n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "        ) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "711758b4-2b10-4e67-8cba-8b7682ccf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937a9cf-5b49-420f-8b7d-c7261ce79e4a",
   "metadata": {},
   "source": [
    "Custom reconstruction loss (mean squared difference between the reconstruction and inputs) adding reconstruction loss to main loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b646b0f-6f75-443f-bd03-e538397ccbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d20da-ebbd-4504-951b-f3f9d540bd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (pyenv)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
