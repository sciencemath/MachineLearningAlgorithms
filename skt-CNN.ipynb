{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e75c23b-c48f-432e-977a-f15997191368",
   "metadata": {},
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94757651-15a0-4b72-ab0f-a22be7db3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "\n",
    "images = load_sample_images()[\"images\"]\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale=1 / 255)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12f47bf-c150-4e01-a602-3b8b251ff98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936797a6-ce93-464b-aec8-ebb5c2cb991f",
   "metadata": {},
   "source": [
    "^ 4D tensor two sample iamges, width, height, channel (red, green blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d340c5-25b6-4236-b7ee-00c8c9267543",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78177f9-0fe4-42ee-9524-20ccc5c6fd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64, 114, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c1f97-4167-41d7-a7e6-30ed058f3a05",
   "metadata": {},
   "source": [
    "OutputSize = InputSize − (KernelSize − 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87709e6b-962c-4beb-8479-5c6849ca8c07",
   "metadata": {},
   "source": [
    "^ height/width shrunk because we lose 3 pixels on each side, 32 feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f9000b-9b88-4d9b-8a73-43824b5eb95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\")\n",
    "fmaps = conv_layer(images)\n",
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328d03e0-afc2-4e99-b9ab-8ad20311be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels, biases = conv_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "642b2f0e-3539-4bf8-a7c5-51cf4b1e6327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8dabdd8-ed00-47a6-afe5-477e9ad1a63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7da57d-4c66-4729-b11f-87863486bf72",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "if we have 200 5×5 filters and we have a 150×100 RGB image: each 200 contains 150×100 neurons: 200(5×5×3×150×100) = 225 million float\n",
    "\n",
    "convolutional layer’s output will occupy 200×150×100×32 = 96 million bits (12 MB) of RAM, 100 instances = 1.2GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30629eda-6fb2-47fc-bca3-6ea0fe43eefd",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "allows us to subsample an image (shrink) reducing computational load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d2e5b-ab2e-4c0e-8723-6a44c0e639fe",
   "metadata": {},
   "source": [
    "people now only use max pooling, we can implement one as Keras does not include a depthwise pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f80b0ebb-cdc6-4ce9-8359-cc89bd98bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        groups = shape[-1]\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1829f9b-ec75-4359-931b-1d1ab624f364",
   "metadata": {},
   "source": [
    "CNN for Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e5fefba-f1ec-462c-b488-4e24db7c59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa93f79-53d6-46d7-9b40-c798d309daaa",
   "metadata": {},
   "source": [
    "It is a common practice to double the number of filters after each pooling layer: since a pooling layer divides each spatial dimension by a factor of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe8da66-7af7-4d8a-97be-e6f6ae9210ba",
   "metadata": {},
   "source": [
    "Data augmentation is useful when you have an unbalanced dataset, generate more samples of the less frequent classes. This is called the synthetic minority oversampling technique (SMOTE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc4c5c-f77d-48a3-ae01-030b7f477090",
   "metadata": {},
   "source": [
    "### ResNet-34 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44316e76-37b9-4159-8a22-9ad2a7fa3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(\n",
    "    tf.keras.layers.Conv2D,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    use_bias=False\n",
    ")\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d0cfb2-3723-4a64-ac38-7d302a7e55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be56f7-3b64-4fbf-9186-489818b9877e",
   "metadata": {},
   "source": [
    "We won't have to do this manually of course Keras comes with pre-trained models, make sure you have images that match the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff1538b4-f443-447a-b2d6-b670a0d46d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# make sure your images are 224 × 224\n",
    "model = tf.keras.applications.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74d974-cb94-41f4-a206-7f003e2f34f6",
   "metadata": {},
   "source": [
    "Resizing after cropping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08e6f29a-9b6d-49f6-8bfb-c8a6ec010997",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_sample_images()[\"images\"]\n",
    "images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f4875-7ce0-4dd4-bd89-3e391d01ed97",
   "metadata": {},
   "source": [
    "most models have a `preprocess_input()` function to preprocess images, some expect 0 to 1, or -1 to 1, here its simply 0 to 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9063706-176b-4bf0-ad09-8af73571c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd30d4f5-7d40-4198-a8b2-8699dbd7e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = model.predict(inputs)\n",
    "y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed8245fa-52c9-473f-81de-557657382f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
      "Image #0\n",
      "  n03598930 - jigsaw_puzzle 30.68%\n",
      "  n02782093 - balloon      17.17%\n",
      "  n03888257 - parachute    5.57%\n",
      "Image #1\n",
      "  n04209133 - shower_cap   34.37%\n",
      "  n09229709 - bubble       11.41%\n",
      "  n02782093 - balloon      9.46%\n"
     ]
    }
   ],
   "source": [
    "top_K = tf.keras.applications.resnet50.decode_predictions(y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(f\"Image #{image_index}\")\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f\"  {class_id} - {name:12s} {y_proba:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2037871-6f25-4f6e-9908-3c9c99eb90cd",
   "metadata": {},
   "source": [
    "### Pretrained models for transfer learning\n",
    "\n",
    "if you don't have enough data to train from scratch we can use lower layers of a pretrained model. Example flower dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ccc39ee-2f27-4767-8c7e-364f49e9171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43936a14-527a-4d0a-80cb-5b954cf970f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e95bd807-69f2-48ab-a5d4-10b9ce9ec765",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])\n",
    "\n",
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0ffea-4fce-4fbe-aac1-f1dd7232f2c0",
   "metadata": {},
   "source": [
    "32 images not very large we can use data augemntation to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59728c1b-41c9-4bd8-92e0-2b82eb322e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91114a1d-ec4c-4558-a8e5-d902281b4843",
   "metadata": {},
   "source": [
    "Xception model, pretrained on ImageNet, note the `include_top=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b09632f-63ed-4b62-8209-39556a834b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e117712-4d16-49a6-94a5-f9e6e519ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "095cda92-49bf-4cdc-8d1a-063275327889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 849ms/step - accuracy: 0.7992 - loss: 1.0871 - val_accuracy: 0.7641 - val_loss: 1.1317\n",
      "Epoch 2/3\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 816ms/step - accuracy: 0.8809 - loss: 0.5196 - val_accuracy: 0.8022 - val_loss: 0.8470\n",
      "Epoch 3/3\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 835ms/step - accuracy: 0.8954 - loss: 0.4275 - val_accuracy: 0.8512 - val_loss: 0.7357\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32552e7-d9a5-4835-b73f-5a6ba540d99c",
   "metadata": {},
   "source": [
    "top layers are now well trained, lets unfreeze layers 56 and above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5491f8fa-c2a1-43fb-a76a-726dcdaa6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70ab7d-1ffd-4398-a232-b76d08101311",
   "metadata": {},
   "source": [
    "compile model whenever freeze or unfreeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc0639a1-5761-49bc-b83b-7bb2d955ba49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.8921 - loss: 0.3169 - val_accuracy: 0.8730 - val_loss: 0.5412\n",
      "Epoch 2/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9677 - loss: 0.1009 - val_accuracy: 0.9220 - val_loss: 0.2717\n",
      "Epoch 3/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.9921 - loss: 0.0217 - val_accuracy: 0.9056 - val_loss: 0.3381\n",
      "Epoch 4/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9931 - loss: 0.0239 - val_accuracy: 0.9201 - val_loss: 0.3091\n",
      "Epoch 5/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9959 - loss: 0.0160 - val_accuracy: 0.9147 - val_loss: 0.3266\n",
      "Epoch 6/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9954 - loss: 0.0160 - val_accuracy: 0.9074 - val_loss: 0.3712\n",
      "Epoch 7/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.9962 - loss: 0.0149 - val_accuracy: 0.9147 - val_loss: 0.3496\n",
      "Epoch 8/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.9928 - loss: 0.0191 - val_accuracy: 0.9238 - val_loss: 0.3096\n",
      "Epoch 9/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9983 - loss: 0.0083 - val_accuracy: 0.9256 - val_loss: 0.3141\n",
      "Epoch 10/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 2s/step - accuracy: 0.9984 - loss: 0.0060 - val_accuracy: 0.9383 - val_loss: 0.2818\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f981d5-d7e3-4daa-8f19-263b716fd170",
   "metadata": {},
   "source": [
    "Understanding where in the image are objects (localizing) can be expressed as a regression task to predict a bounding box: horizontal, vertical coordinates of center as well as width and height (4 items to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6360f59-9c23-4b3f-9ac6-8cf4406b8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = tf.keras.layers.Dense(4)(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=[class_output, loc_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "    loss_weights=[0.8, 0.2], # depends on what you care most about\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0130e4-47e2-45d3-bc2d-44a2eb97058e",
   "metadata": {},
   "source": [
    "The flower dataset does not have bounding boxes around the flowers, so we need to add them ourselves. This is often one of the hardest most costly parts of a machine learning project (getting the labels)\n",
    "\n",
    "Look for tools that do this like VGG, Image Annotator, Amazon Mechanical Turk, etc. could also use a crowdsourcing platform.\n",
    "\n",
    "Each item should be of the form tuple: `(images, (class_labels, bounding_boxes))`\n",
    "\n",
    "Its better to predict the square root of the width and height rather than getting them directly:\n",
    "\n",
    "a 10 pixel error for a large bounding box will not be penalized as much as a 10-pixel error for a small bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82fec7e-89b6-43f2-9919-cee7c4a3520d",
   "metadata": {},
   "source": [
    "### Intersection over union (IoU)\n",
    "\n",
    "this metric measures how well our prediction for the bounding boxes went, overlap between predicted bounding box and target bounding box divided by the area of their union\n",
    "`tf.keras.metrics.MeanIoU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b1208-a44d-478d-8c66-b4e9c6b21c26",
   "metadata": {},
   "source": [
    "### Mean Average Precision (mAP)\n",
    "\n",
    "Mean Average Precision (mAP) is a key metric for evaluating object detection models. It builds on precision and recall, which measure how well a model identifies objects correctly.\n",
    "\n",
    "Average Precision (AP): Instead of taking precision at a fixed recall level, we compute the maximum precision at each recall threshold (0%, 10%, 20%, ..., 100%) and then average these values.\n",
    "\n",
    "Mean Average Precision (mAP): When dealing with multiple object classes, we calculate AP for each class, then take the mean of all these AP values.\n",
    "\n",
    "Bounding Box Accuracy (IoU Thresholds): In object detection, predictions must be both correct in class and correctly localized. We use Intersection over Union (IoU) to measure this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ada41-478b-430f-9438-91b14c3b1586",
   "metadata": {},
   "source": [
    "CNNs are so vast and moving quickly, we have to explore more later (video, object segmentation, predicting next frame in video, combining text and images)\n",
    "\n",
    "https://www.tensorflow.org/hub/tutorials/tf2_object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5738db-b956-4f98-9281-eaf5adc9cad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (pyenv)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
