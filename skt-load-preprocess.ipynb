{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d6313c-4606-403f-8a08-976060bc4d5b",
   "metadata": {},
   "source": [
    "### Usually data is loaded from disk, here we'll use a Tensor using tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22cce153-45d1-4ece-9dad-fa0d7e587040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815e64a9-3ad4-4604-b447-f8ea29fd634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def iterate_dataset(ds):\n",
    "    for item in ds:\n",
    "        tf.print(item)  # Use tf.print() for TensorFlow graphs\n",
    "\n",
    "iterate_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76204f1d-0de9-4fb7-a0db-fdefd3953cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    }
   ],
   "source": [
    "# Keeps nested structure\n",
    "X_nested = {\"a\": ([1,2,3], [4,5,6]), \"b\": [7,8,9]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1c4e49-3c83-4d52-9c5e-daccb53f0a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Chaining transformations, batch with `drop_remainder=True` will remove the last\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a3130a-254d-4f0e-94f3-701f0f97598b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b0f296-8ef0-4d27-a3d2-b97851a85d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 16 18 ... 2 4 6]\n",
      "[8 10 12 ... 16 18 0]\n",
      "[2 4 6 ... 10 12 14]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "for item in dataset:\n",
    "    tf.print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d3db784-801d-4ebe-964c-cef9f1d30ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8aa821-7ff5-4240-bb18-2a25e6378d07",
   "metadata": {},
   "source": [
    "Shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8e333c-ff86-4021-9561-8e03d2c7fe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d278773-b268-462e-afae-9a9c456b9d06",
   "metadata": {},
   "source": [
    "Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea6120a0-8708-4aa6-9c46-675abb268ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = ['./data/housing/housing.csv', './data/housing/housing_train_01.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe11eb-fc46-4d2e-b620-5a90679c9058",
   "metadata": {},
   "source": [
    "`list_files()` shuffles filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f25baaa-3926-4bfa-a8f6-7d4f391f1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2846ce-eba5-446f-9796-033bc9ca714f",
   "metadata": {},
   "source": [
    "Skipping the header and taking 5 TextLineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c62ae52e-4133-4d4d-bbdf-07dea1589574",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88a5ff1a-108d-4666-8811-abbba6ce66f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'-122.23,37.88,41.0,880.0,129.0,322.0,126.0,8.3252,452600.0,NEAR BAY', shape=(), dtype=string)\n",
      "tf.Tensor(b'-122.23,37.88,41.0,880.0,129.0,322.0,126.0,8.3252,452600.0,NEAR BAY', shape=(), dtype=string)\n",
      "tf.Tensor(b'-122.22,37.86,21.0,7099.0,1106.0,2401.0,1138.0,8.3014,358500.0,NEAR BAY', shape=(), dtype=string)\n",
      "tf.Tensor(b'-122.22,37.86,21.0,7099.0,1106.0,2401.0,1138.0,8.3014,358500.0,NEAR BAY', shape=(), dtype=string)\n",
      "tf.Tensor(b'-122.24,37.85,52.0,1467.0,190.0,496.0,177.0,7.2574,352100.0,NEAR BAY', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264b87b-1766-4d30-84ca-2d231481c97b",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e1f528-3326-4729-979d-5b9e5d47dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate sample training data (1000 rows, 8 features)\n",
    "np.random.seed(42)\n",
    "X_train_sample = np.random.rand(1000, 8) * 10  # Random values between 0 and 10\n",
    "\n",
    "# Compute mean and std using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_sample)  # Compute mean and std\n",
    "\n",
    "X_mean = scaler.mean_  # Mean of each feature\n",
    "X_std = scaler.scale_  # Standard deviation of each feature\n",
    "\n",
    "n_inputs = 8\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "def preprocess(line):\n",
    "    x, y = parse_csv_line(line)\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bceb531-de39-491c-bf09-495f6e2d4743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([-2.8835964e-01,  1.3298166e+01,  1.8436758e-01, -1.4383570e+00,\n",
       "         2.9296912e+02, -9.6995091e-01,  1.1021225e+01, -4.3763279e+01],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dffc41-aef6-460f-a2dd-b0b67afb16f7",
   "metadata": {},
   "source": [
    "everything together: `prefetch()` for performance, `num_parallel_calls` multiple CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a287bcec-9302-41a5-a0d3-c972c65d008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n",
    "                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n",
    "                       batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size).prefetch(1) # works in parallel getting the next batch ready while processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5ee3b-4c3b-4965-8130-b932c500da96",
   "metadata": {},
   "source": [
    "TF function that trains model for whole epoch (speeds up training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e75fddbc-ec1d-4c49-bbb9-58fc9b5781fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "@tf.function\n",
    "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
    "    train_one_epoch(model, optimizer, loss_fn, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c90355-275a-497a-bef3-fec2f0e482b5",
   "metadata": {},
   "source": [
    "### TFRecord format is TensorFlow's preferred format\n",
    "\n",
    "holds sequence of binary records, and CRC checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "359a732f-38e4-4f98-8e2d-958ae5973e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"...and this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ff99640-898d-4638-a2fc-de57ec6551b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'...and this is the second record', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 08:49:51.669677: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:370] TFRecordDataset `buffer_size` is unspecified, default to 262144\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a670d-01eb-4249-b120-c80cf187b18e",
   "metadata": {},
   "source": [
    "Instead of writing raw bytes, use TensorFlow's tf.train.Example format to store structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b021e17-6c43-4b4b-b9cc-588f97ddd72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first record\n",
      "...and this is the second record\n"
     ]
    }
   ],
   "source": [
    "# Create a TFRecord file with structured data\n",
    "with tf.io.TFRecordWriter(\"my_data2.tfrecord\") as writer:\n",
    "    for text in [b\"This is the first record\", b\"...and this is the second record\"]:\n",
    "        feature = {\n",
    "            \"text\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[text]))\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        writer.write(example.SerializeToString())  # Serialize and write\n",
    "\n",
    "# Read TFRecord file\n",
    "def parse_example(example_proto):\n",
    "    feature_description = {\n",
    "        \"text\": tf.io.FixedLenFeature([], tf.string)  # Define expected structure\n",
    "    }\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return parsed_example[\"text\"]\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_data2.tfrecord\"])\n",
    "dataset = dataset.map(parse_example)  # Decode each record\n",
    "\n",
    "# Print the processed dataset\n",
    "for item in dataset:\n",
    "    print(item.numpy().decode(\"utf-8\"))  # Convert back to a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ff560-ed53-4cb1-80be-a69a0b793a62",
   "metadata": {},
   "source": [
    "Compress Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec4d065e-7e95-4adc-a1b9-508803a84e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"Compress, compressing, compressed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "040fc188-112b-4ede-96dd-40a82bed5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"], compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ae56d-bfff-4ee2-95fd-3730a20ffb28",
   "metadata": {},
   "source": [
    "### protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee0cc44e-2091-4948-80a3-18cf7dbe3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Mathias\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[420])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bdd35-4e92-44e7-ba70-17b90498fd2d",
   "metadata": {},
   "source": [
    "^ this could be a helper function, as its a bit verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ac3d0bf-e669-4ab4-b1b1-fb815e3e7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    for _ in range(5):\n",
    "        f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec577c0e-b6d9-404f-b601-c29dda07ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=420>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Mathias'>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=420>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Mathias'>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=420>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Mathias'>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=420>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Mathias'>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=420>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Mathias'>}\n"
     ]
    }
   ],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse(serialized_example):\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).map(parse)\n",
    "for parsed_example in dataset:\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7a2cfcb-2308-444a-9a52-f79c76bb2472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e72c1241-4d0a-483c-b6e9-057ab071224f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example[\"emails\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82cd75-76bc-47e5-8fdc-630d84e37f55",
   "metadata": {},
   "source": [
    "Batch parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "305c7286-476e-41d7-81d8-f8e25e5c528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]], shape=(4, 2), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com' b'a@b.com' b'c@d.com'], shape=(4,), dtype=string), dense_shape=tf.Tensor([2 2], shape=(2,), dtype=int64)), 'id': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([420, 420])>, 'name': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Mathias', b'Mathias'], dtype=object)>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]], shape=(4, 2), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com' b'a@b.com' b'c@d.com'], shape=(4,), dtype=string), dense_shape=tf.Tensor([2 2], shape=(2,), dtype=int64)), 'id': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([420, 420])>, 'name': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Mathias', b'Mathias'], dtype=object)>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([1 2], shape=(2,), dtype=int64)), 'id': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([420])>, 'name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Mathias'], dtype=object)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 09:26:29.261646: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def parse(serialized_examples):\n",
    "    return tf.io.parse_example(serialized_examples, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(2).map(parse)\n",
    "for parsed_examples in dataset:\n",
    "    print(parsed_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac29e0-cd33-4eb3-8c38-cb9993ae2711",
   "metadata": {},
   "source": [
    "Its best to have any preprocessing layers directly in your model, so its easily portable to production and it can preprocess data on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d069623-d040-4062-89c7-ca58517ba0de",
   "metadata": {},
   "source": [
    "measure feature means and variances before training\n",
    "\n",
    "This eliminates the risk of preprocessing mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5309351e-abe8-4986-9782-7b1ac1c72125",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "model = tf.keras.models.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "# norm_layer.adapt(X_train)\n",
    "# model.fit(X_train, y_trian, validation_data=(X_valid, y_valid), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc7a67-bb79-4f6c-ac7d-782901285661",
   "metadata": {},
   "source": [
    "normalizing the whole training set just once before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d778ac6-1a74-41ee-8e40-c41e281deff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "# norm_layer.adapt(X_train)\n",
    "# X_train_scaled = norm_layer(X_train)\n",
    "# X_valid_scaled = norm_layer(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239fb6e5-f0b9-4a62-93a3-b4d3de621920",
   "metadata": {},
   "source": [
    "train model on scaled data without Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4521e899-91ee-49cf-b5c3-0b73274767ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "# model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1216ef-2078-44ff-a4dd-455481baaaed",
   "metadata": {},
   "source": [
    "we need to prepocess the inputs when deploying to production:\n",
    "\n",
    "this will take care of both preprocessing its inputs and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85b6b523-3e6f-4291-b1fd-3647f46463dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = tf.keras.Sequential([norm_layer, model])\n",
    "# X_new = X_test[:3]\n",
    "# y_pred = final_model(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a0dad4-00a8-43c3-b6a4-14f17538e84b",
   "metadata": {},
   "source": [
    "adapted normalization layer to input features of each batch in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "793a78d0-470c-48b1-be88-9037406c227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(lambda X, y: (norm_layer(X), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6183867-b707-4cdc-9456-e35cfaed4be0",
   "metadata": {},
   "source": [
    "Custom layer, if more features needed than Keras preprocessing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69f84294-98bf-4629-899f-889a894c0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyNormalization(tf.keras.layers.Layer):\n",
    "    def adapt(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
    "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        return (inputs - self.mean_) / (self.std_ + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b02ddb-4add-487a-b0c2-08a08cefeb08",
   "metadata": {},
   "source": [
    "### Discretization layer\n",
    "\n",
    "numreical features into categorical features by mapping value ranges (bins) to categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a945c-3d89-432c-8805-cf65e3536c10",
   "metadata": {},
   "source": [
    "less than 18, 18 to 50 (not included), 50 and over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51b685e8-1074-4867-9b5f-f16343fb01fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n",
    "discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40edef-6fc6-4876-81e6-bee09bc92f3a",
   "metadata": {},
   "source": [
    "we can do it based on percentiles `num_bins=3` values below the 33rd and 66th percentiles (10 and 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc95ed54-35ef-4ce7-a022-964130340dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
    "discretize_layer.adapt(age)\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e182c4-b6f2-4946-b4c0-fd6a1d53ee21",
   "metadata": {},
   "source": [
    "should not be used for neural network, we can use one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e7417b4-68e0-4cf5-bf96-3800c7174c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
    "onehot_layer(age_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb69b8-3359-4fcf-bbd3-58f8904a1a83",
   "metadata": {},
   "source": [
    "encoding more than one categorical feature at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a19841f6-cb55-49f2-9035-d4c99737f99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])\n",
    "onehot_layer(two_age_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532431fc-6c93-4af4-83ec-33a9c13f8597",
   "metadata": {},
   "source": [
    "`output_mode=\"count\"` how many times each category occured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc0167-b1df-42b0-9fc3-4fdda86448a9",
   "metadata": {},
   "source": [
    "Multi-hot encoding lose information both [0, 1] amd [1, 0] are encoded as [1, 1, 0] we can avoid this by one-hot encode each feature separately and concat the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b856c03-80cf-4914-b687-fd333e2f7cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)\n",
    "onehot_layer(two_age_categories + [0, 3]) # adds 3 to the second feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb83fc7-2c04-4782-9985-7516de6d1554",
   "metadata": {},
   "source": [
    "^ first three columns correspond to the first feature, and the last three correspond to the second feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da613576-3217-4cfb-b023-0d8b3ed2eddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = [\"Auckland\", \"Paris\", \"Paris\", \"Santa Monica\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31a3d3-0e54-4aca-a9a0-b87e743e995c",
   "metadata": {},
   "source": [
    "one-hot vector instead of an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1f58134-9ef3-44e1-bdf6-d4cd959c055b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\n",
       "array([[0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0]])>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb63681-875d-47d8-8492-988fc0212211",
   "metadata": {},
   "source": [
    "out-of-vocabulary (OOV) buckets: each unknown category will get mapped pseudorandomly to one OOV buckets, using a hash function modulo the number of OOV buckets. This will allow the model to distinguish at least some of the rare categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "493f81a7-b9b4-4c2a-a062-1fe14316c4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[5],\n",
       "       [7],\n",
       "       [4],\n",
       "       [3],\n",
       "       [4]])>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Foo\"], [\"Bar\"], [\"Baz\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04df134-638d-4366-840e-7fb1e8ef62e7",
   "metadata": {},
   "source": [
    "### Hashing layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7056fea7-2c24-43b2-ac23-b5f76b73f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [9],\n",
       "       [1]])>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_layer = tf.keras.layers.Hashing(num_bins=10)\n",
    "hashing_layer([[\"Paris\"], [\"Tokyo\"], [\"Auckland\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64963f2-e7be-4b70-a82d-de7811feb07d",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "encoding some categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86d0beb2-c468-4d97-9155-34917f8023df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.0215165 ,  0.03553423],\n",
       "       [-0.02882874, -0.0356148 ],\n",
       "       [-0.0215165 ,  0.03553423]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
    "embedding_layer(np.array([2, 4, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a6b66-98d1-47d8-82a9-4c18db782930",
   "metadata": {},
   "source": [
    "Embedding layer is initialized randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc576339-3254-41fd-9d90-6c492b7f966d",
   "metadata": {},
   "source": [
    "embedding categorical text attribute, chain StringLookup layer and Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c188d54-6023-4381-88d5-c8609b1ba10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(ocean_prox)\n",
    "lookup_and_embed = tf.keras.Sequential([\n",
    "    str_lookup_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(), output_dim=2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8db0e1cc-27f9-4bca-a143-8b83b5503738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 2), dtype=float32, numpy=\n",
       "array([[[-0.00636964, -0.0307073 ]],\n",
       "\n",
       "       [[ 0.02456259,  0.01703635]],\n",
       "\n",
       "       [[-0.00636964, -0.0307073 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_and_embed(np.array([[\"<1H OCEAN\"], [\"ISLAND\"], [\"<1H OCEAN\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea48ecb-6261-49ac-a7e9-d1a10007098c",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8bd2022b-4119-4061-90bf-facaaecbc08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
       "array([[2, 1, 0, 0],\n",
       "       [6, 2, 1, 2]])>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
    "text_vec_layer = tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f0cd0-b8ab-4933-9903-e120ddf2ace2",
   "metadata": {},
   "source": [
    "^ since first sentence is sort it is padded with 0's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8738da-9ed4-458c-bd03-d69f47e4a095",
   "metadata": {},
   "source": [
    "preferable to set \"tf_idf\", which stands for term-frequency × inverse-document-frequency (TF-IDF). This is similar to the count encoding, but words that occur frequently in the training data are downweighted, rare words are upweighted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a9ad020-44a3-460c-a0a0-f70a38ca1afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
       "array([[0.96725637, 0.6931472 , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.96725637, 1.3862944 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 ]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c9d48-843a-4ca3-a842-fb3f2d062a74",
   "metadata": {},
   "source": [
    "weight equal to `log(1 + d / (f + 1))`\n",
    "\n",
    "d: total number of sentences\n",
    "\n",
    "f: counts how many of training sentences contain given word\n",
    "\n",
    "Since the word \"be\" occurs twice in the sentence \"Question: be or be?\", it gets encoded as 2 × log(1 + 4 / (1 + 3)) ≈ 1.3862944."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4bd6d-abe3-4531-8df1-41a5d67528cb",
   "metadata": {},
   "source": [
    "### Tensorflow hub (not sure if people are using this over HuggingFace?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8873ea17-0d7e-48b8-85c2-fd28a4d09225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_hub) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_hub) (4.25.6)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow_hub)\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tf-keras>=2.14.1->tensorflow_hub) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (24.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow_hub) (0.1.2)\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras, tensorflow_hub\n",
      "Successfully installed tensorflow_hub-0.16.1 tf-keras-2.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6cec3c-41a4-4f83-a8a7-1cc3353ee55f",
   "metadata": {},
   "source": [
    "sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f5a7b05d-b85f-476f-b47b-c663edc30a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25,  0.28,  0.01,  0.1 ,  0.14,  0.16,  0.25,  0.02,  0.07,\n",
       "         0.13, -0.19,  0.06, -0.04, -0.07,  0.  , -0.08, -0.14, -0.16,\n",
       "         0.02, -0.24,  0.16, -0.16, -0.03,  0.03, -0.14,  0.03, -0.09,\n",
       "        -0.04, -0.14, -0.19,  0.07,  0.15,  0.18, -0.23, -0.07, -0.08,\n",
       "         0.01, -0.01,  0.09,  0.14, -0.03,  0.03,  0.08,  0.1 , -0.01,\n",
       "        -0.03, -0.07, -0.1 ,  0.05,  0.31],\n",
       "       [-0.2 ,  0.2 , -0.08,  0.02,  0.19,  0.05,  0.22, -0.09,  0.02,\n",
       "         0.19, -0.02, -0.14, -0.2 , -0.04,  0.01, -0.07, -0.22, -0.1 ,\n",
       "         0.16, -0.44,  0.31, -0.1 ,  0.23,  0.15, -0.05,  0.15, -0.13,\n",
       "        -0.04, -0.08, -0.16, -0.1 ,  0.13,  0.13, -0.18, -0.04,  0.03,\n",
       "        -0.1 , -0.07,  0.07,  0.03, -0.08,  0.02,  0.05,  0.07, -0.14,\n",
       "        -0.1 , -0.18, -0.13, -0.04,  0.15]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "# pre-trained on Google News 7B corpus\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
    "sentence_embeddings.numpy().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b2121-a1c2-43c6-ba9d-a7768241b0f9",
   "metadata": {},
   "source": [
    "### Images preprocessing\n",
    "\n",
    "`tf.keras.layers.Rescaling(scale=2/255, offset=-1)` scaled the values from 0 -> 255 to -1 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea819e11-9707-4c37-b0e7-e2d7f4942f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "images = load_sample_images()[\"images\"]\n",
    "crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)\n",
    "crapped_images = crop_image_layer(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398e769-f44a-401b-beae-1024ae2ef8a4",
   "metadata": {},
   "source": [
    "preprocessing layers are based on TF low-level API most of these methods in this notebook call a different method for example: `Normalization` -> `tf.nn.moments()`, `Discretization` -> `tf.raw_ops.Bucketize()` etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6fa8a270-5fde-4f89-925d-2b8cdfa9492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: absl-py in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Collecting click (from tensorflow_datasets)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.9.tar.gz (35 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting immutabledict (from tensorflow_datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_datasets) (2.0.2)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_datasets) (4.25.6)\n",
      "Requirement already satisfied: psutil in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_datasets) (6.1.1)\n",
      "Collecting pyarrow (from tensorflow_datasets)\n",
      "  Downloading pyarrow-19.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Collecting simple-parsing (from tensorflow_datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_datasets) (2.5.0)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow_datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: wrapt in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tensorflow_datasets) (1.17.2)\n",
      "Requirement already satisfied: etils>=1.9.1 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.11.0)\n",
      "Requirement already satisfied: fsspec in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (2025.1.31)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from dm-tree->tensorflow_datasets) (25.1.0)\n",
      "Requirement already satisfied: six in /Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from promise->tensorflow_datasets) (1.17.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow_datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-19.0.0-cp312-cp312-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Building wheels for collected packages: dm-tree, promise\n",
      "  Building wheel for dm-tree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dm-tree: filename=dm_tree-0.1.9-cp312-cp312-macosx_15_0_arm64.whl size=112050 sha256=c65c29d23e5b346f25a99bdfc903eaabdeda5c1c2bcb78ee9f1db8ea8a413582\n",
      "  Stored in directory: /Users/mathias/Library/Caches/pip/wheels/89/01/af/d33b8877c63c2b90659a0eba4cffefbd049665c311d0a3d23a\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21548 sha256=7b578e5f1ccdab9128ed3326f3e45a0da0c795ceb60d7e6daed21ac7af18bfb4\n",
      "  Stored in directory: /Users/mathias/Library/Caches/pip/wheels/e7/e6/28/864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built dm-tree promise\n",
      "Installing collected packages: tqdm, toml, pyarrow, promise, immutabledict, googleapis-common-protos, docstring-parser, dm-tree, click, tensorflow-metadata, simple-parsing, tensorflow_datasets\n",
      "Successfully installed click-8.1.8 dm-tree-0.1.9 docstring-parser-0.16 googleapis-common-protos-1.66.0 immutabledict-4.2.1 promise-2.3 pyarrow-19.0.0 simple-parsing-0.1.7 tensorflow-metadata-1.16.1 tensorflow_datasets-4.9.7 toml-0.10.2 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ebf85da8-be3f-4d20-a4d0-d5d54938ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 19:39:34.961788: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/mathias/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cd32ae145f4a58ab495038ff0a0bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/mathias/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9896487e-8624-44c2-8866-a6a07b648a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 19:40:35.985234: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
     ]
    }
   ],
   "source": [
    "for batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ee576-278c-4b72-b08a-a92ddc7f7db2",
   "metadata": {},
   "source": [
    "Keras expects each item to be a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3356f5b9-7a9f-4ffe-a673-aacdf22f9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(buffer_size=10_000, seed=42).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97bc4eaf-0d83-4743-af41-105df600c522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathias/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 699us/step - accuracy: 0.7631 - loss: 18.5642 - val_accuracy: 0.8822 - val_loss: 6.0332\n",
      "Epoch 2/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537us/step - accuracy: 0.8783 - loss: 5.6927 - val_accuracy: 0.8788 - val_loss: 5.6601\n",
      "Epoch 3/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539us/step - accuracy: 0.8843 - loss: 5.2441 - val_accuracy: 0.8818 - val_loss: 5.8123\n",
      "Epoch 4/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550us/step - accuracy: 0.8898 - loss: 4.9820 - val_accuracy: 0.8845 - val_loss: 5.6028\n",
      "Epoch 5/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558us/step - accuracy: 0.8894 - loss: 4.7905 - val_accuracy: 0.8672 - val_loss: 6.2826\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - accuracy: 0.8776 - loss: 5.9887\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set = tfds.load(\n",
    "    name=\"mnist\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "train_set = train_set.shuffle(buffer_size=10_000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = valid_set.batch(32).cache()\n",
    "test_set = test_set.batch(32).cache()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c6fb5-c993-4a12-ad48-5fe1b5d3e451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (pyenv)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
